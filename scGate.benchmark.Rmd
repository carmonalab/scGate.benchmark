---
title: "Small benchmark of cell type isolation with scGate and other methods"
author: "M. Andreatta, A. Berenstein and S. Carmona"
date: "29/09/2021"
output:
  rmdformats::readthedown:
    self-contained: true
    highlight: haddock
    thumbnails: false
    css: styles.css
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file, encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'scGate.bench.html'))})
---


```{r, message=F, warning=F,results=F}
library(renv)
renv::activate()
renv::restore()
#remotes::install_github("carmonalab/UCell", ref="v1.1")
#remotes::install_github("mojaveazure/seurat-disk",ref="163f1aade5bac38ed1e9e9c912283a7e74781610")
#remotes::install_github("carmonalab/scGate", ref='v1.0.0') #stable release
#install.packages("SCINA")
#install.packages("BiocManager")
#BiocManager::install("devtools")
#library(devtools)
#BiocManager::install(c('DelayedArray', 'DelayedMatrixStats',"monocle","org.Hs.eg.db","org.Mm.eg.db"))
#devtools::install_github("cole-trapnell-lab/garnett")


library(ggplot2)
library(dplyr)
library(scGate)
library(SingleR)
library(SCINA)
library(garnett)
library(org.Hs.eg.db)

```


## Load some testing datasets and models

Load datasets (you will need ~ 14gb of RAM)
```{r}
# Load Jerby et.al, Yost et.al, Zilionis et.al, and Hao et.al datasets
rds.file = "./aux/human_sets_benchmark.rds"
if (!file.exists(rds.file)) {
  options(timeout=2000)
  download.file("https://figshare.com/ndownloader/files/31346815?private_link=7d4d9376a3bbdb17bc00", rds.file)
}
testing.datasets <- readRDS(rds.file) 

# Load Abdelaal 2019 pbmc data (also used in JiaruiDing2020)
rds.file.abdelaal = "./aux/pbmc_abdelaal2019_processed.rds"
if (!file.exists(rds.file.abdelaal)) {
  options(timeout=2000)
  download.file("https://figshare.com/ndownloader/files/33846359?private_link=c84ef361399e2f5148d8", rds.file.abdelaal)
}
abdelaal.pbmc.data <- readRDS(rds.file.abdelaal)
testing.datasets <- c(testing.datasets, abdelaal.pbmc.data)
dset.to.run <- names(testing.datasets)

ground_truth_categories <- readRDS("./aux/goldStandards.rds")  #Load an object containing standardized cell type annotations - to allow comparing different methods
figure.path <- "./plots/"
dir.create(figure.path, showWarnings = F)

results <- data.frame()
```


## Run models and compute performance

Run scGate
```{r,collapse =T, results= F, echo = F}
#Load default models from DB
models.DB <- scGate::get_scGateDB(version="v0.4")  ### Specifying a version ensure reproducibility of signatures
models <- models.DB$human$generic  ## Pre-compiled list of models for human cell types
print(models%>%names)

#skip these models since there is no equivalent class in HPCA for SingleR
models <- models[!names(models) %in% c("Tcell.alphabeta","Plasma_cell","PanBcell")] 

results.scGate <- data.frame()

for(dset in dset.to.run){
  message(sprintf('-Running %s',dset))
  obj <- testing.datasets[[dset]]
  assay <-  DefaultAssay(obj)
  

  for(model.name in names(models)){
    if(sum(results$method == "scGate" & dset %in% results$dataset & model.name %in% results$model) == 3) next
    
    message(sprintf('---Running %s',model.name))
    model <- models[[model.name]]
    obj <- scGate(data = obj, model = model, ncores = 4, keep.ranks = TRUE, output.col.name = paste0('is.pure.',model.name), assay = assay)
    
    if (model.name %in% colnames(obj@meta.data)) {
      res <- scGate::performance.metrics(actual = obj@meta.data[,model.name], pred = obj@meta.data[,paste0("is.pure.",model.name)] == "Pure")
      res <- data.frame(value = res, metric = names(res),model = model.name,dataset = dset,method = "scGate")
      results.scGate <- rbind(results.scGate,res)
    }
  }
}
```


Run SingleR
```{r}
library(celldex)
hpca.se <- HumanPrimaryCellAtlasData() 
param <- BiocParallel::MulticoreParam(workers = 4)

res.SingleR <- list()
SingleR.output <- list()
results.singleR <- data.frame()

for(dset in dset.to.run){
    obj <- testing.datasets[[dset]]
    SingleR.output[[dset]] <- SingleR(test = GetAssayData(obj), ref = hpca.se,labels = hpca.se$label.fine, BPPARAM=param)
    res.SingleR[[dset]][["SingleR.fine"]] <- SingleR.output[[dset]]$pruned.labels
    
    for(model.name in names(models)){
        model <- models[[model.name]]
        
        pred.singleR <- (res.SingleR[[dset]])$SingleR.fine%>%grepl(ground_truth_categories$hpca_categories[[model.name]],.)+0
    
      if (model.name %in% colnames(obj@meta.data)) {
        res <- scGate::performance.metrics(actual = obj@meta.data[,model.name], pred = pred.singleR == 1)
        res <- data.frame(value = res, metric = names(res),model = model.name,dataset = dset,method = 'SingleR')
        results.singleR <- rbind(results.singleR,res)
      }
    }  
}

results <- rbind(results.scGate,  results.singleR)
```

# Build SCINA model signatures 
```{r}
scina.signatures <- list()
scina.signatures$Bcell <- c("PTPRC","LCK","MS4A1","BANK1","PAX5","CD19","CD79A") 
scina.signatures$Plasma <- c("PTPRC","LCK","IGKC","IGHG3","IGHG1","IGHA1","CD79A") 
scina.signatures$PanBcell <- c("PTPRC","LCK","CD79A")
scina.signatures$MoMacDC <- c("PTPRC","SPI1","LYZ","CSF1R","MSR1","MAFB","CD300E")
scina.signatures$Neutrophils <- c("PTPRC","SPI1","CSF3R","FCGR3B")
scina.signatures$Myeloid <- c("PTPRC","SPI1")
scina.signatures$Tcell <- c("CD3D","CD3E","CD3G","CD2")
scina.signatures$CD4T <- c("CD3D","CD3E","CD3G","CD2","CD4","CD40LG")
scina.signatures$CD8T <- c("CD3D","CD3E","CD3G","CD2","CD8A","CD8B")
scina.signatures$Tgammadelta <- c("CD3D","CD3E","CD3G","CD2","TRDC","TRGC1","TRGC2","TRDV1")
scina.signatures$NK <- c("LCK","KLRD1","NKG7","NCR1","FCGR3A")
scina.signatures$Epithelial <- c("CDH1","FLT1")
scina.signatures$Stromal <- c("MMP2","COL1A1","COL1A2","COL5A1","LUM","PDGFRA")
scina.signatures$Erythrocyte <- c("HBB","HBA2","HBA1")
```

We cannot simply run SCINA with multiple, conflicting signatures, as the method would be forced to assign to one class or another (e.g. myeloid or MoMacDC). Instead, we need to set up a "model" for each target cell type that avoids conflicts.

```{r}
scina.model <- list()
scina.model$Bcell <- scina.signatures[c("Bcell","MoMacDC","Tcell","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$PanBcell <- scina.signatures[c("PanBcell","MoMacDC","Tcell","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$Plasma_cell <- scina.signatures[c("Plasma","MoMacDC","Tcell","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$MoMacDC <- scina.signatures[c("MoMacDC","Bcell","Tcell","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$Myeloid <- scina.signatures[c("Myeloid","Bcell","Tcell","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$Tcell <- scina.signatures[c("Tcell","Bcell","MoMacDC","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$CD4T <- scina.signatures[c("CD4T","Bcell","MoMacDC","CD8T","Tgammadelta","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$CD8T <- scina.signatures[c("CD8T","Bcell","MoMacDC","CD4T","Tgammadelta","NK","Epithelial","Stromal","Erythrocyte")]
scina.model$NK <- scina.signatures[c("NK","Bcell","MoMacDC","Tcell","Epithelial","Stromal","Erythrocyte")]
```

# Run SCINA
```{r}
for(dset in dset.to.run){
  message(sprintf('-Running %s',dset))
  obj <- testing.datasets[[dset]]
  assay <-  DefaultAssay(obj)
  expr.mat <- GetAssayData(object = obj,assay = assay)
#  res.scina <- SCINA(expr.mat,signatures = scina.signatures, max_iter=100,convergence_n=10,convergence_rate=0.99,sensitivity_cutoff=0.9,rm_overlap=F)

  for(model.name in names(models)){
    
    res.scina <- SCINA(expr.mat,signatures = scina.model[[model.name]], rm_overlap = F)

    pred.SCINA <- (res.scina$cell_labels==model.name) + 0
    res.SCINA <- scGate::performance.metrics(actual = obj@meta.data[,model.name], pred = pred.SCINA == 1)
    res.SCINA <- data.frame(value = res.SCINA, metric = names(res.SCINA),model = model.name,dataset = dset,method = 'SCINA')
    results <- rbind(results,res.SCINA)
  }  
}
```



## Run Garnett pre-trained model (pbmc)
Download and load pre-trained classifier
```{r}
download.file("https://cole-trapnell-lab.github.io/garnett/classifiers/hsPBMC_20191017.RDS",destfile = "./aux/garnett_hsPBMC_classifier.RDS")
download.file("https://cole-trapnell-lab.github.io/garnett/marker_files/hsPBMC_markers.txt",destfile = "./aux/garnett_hsPBMC_markerList.txt")
pbmc_classifier <- readRDS("./aux/garnett_hsPBMC_classifier.RDS")  # load pre-trained classifier

```



# Garnett classification
```{r}
for(dset in dset.to.run){
  message(sprintf('-Running %s',dset))
  obj <- testing.datasets[[dset]]
  
  #adapt format
  cds.obj <- as.CellDataSet(obj)
  cds.obj <- estimateSizeFactors(cds.obj)

  #make classif
  cds.obj <- classify_cells(cds.obj, pbmc_classifier,
                             db = org.Hs.eg.db,
                             cluster_extend = TRUE,
                             cds_gene_id_type = "SYMBOL")
  

  for(model.name in names(models)){
    pred.garnett.cellType <- (pData(cds.obj)$cell_type) %>%grepl(ground_truth_categories$garnett.PBMC.clasif.pre.trained[[model.name]],.)+0
    res.garnett <- scGate::performance.metrics(actual = obj@meta.data[,model.name], pred = pred.garnett.cellType == 1)
    res.garnett <- data.frame(value = res.garnett, metric = names(res.garnett),model = model.name,dataset = dset,method = 'Garnett')
    results <- rbind(results,res.garnett)

    pred.garnett.ext.cellType <- (pData(cds.obj)$cluster_ext_type) %>%grepl(ground_truth_categories$garnett.PBMC.clasif.pre.trained[[model.name]],.)+0
    res.garnett.ext <- scGate::performance.metrics(actual = obj@meta.data[,model.name], pred = pred.garnett.ext.cellType == 1)
    res.garnett.ext <- data.frame(value = res.garnett.ext, metric = names(res.garnett.ext),model = model.name,dataset = dset,method = 'Garnett.clust.ext')
    results <- rbind(results,res.garnett.ext)

  }  
}
```
Save results before summarization
```{r}
# save raw results per dataset (without summarizing Abdelaal's results)
saveRDS(results,"aux/results.raw.rds")
```

Summarize Abdelaal results averaging pbmc1 and pbmc2 datasets
```{r}
results.no.abdelaal <- results%>%subset(!dataset %in% c("pbmc1.abdelaal","pbmc2.abdelaal"))

# average performance of pbmc1 and pbmc2 abdelaal datasets
results.abdelaal <- results%>%subset(dataset %in% c("pbmc1.abdelaal","pbmc2.abdelaal"))
results.abdelaal.mean <- results.abdelaal%>%group_by(method,metric,model)%>%summarise(value = mean(value,drop.na =T))
results.abdelaal.mean$dataset <- "Abdelaal"

# merge abdelaal results to the other datasets
results <- rbind(results.no.abdelaal, results.abdelaal.mean)
```

```{r}
results.to.plot <- results%>%subset(method!="Garnett.clust.ext" & metric!= "REC" & model!="PanBcell")
#results.to.plot <- results%>%subset(method!="Garnett.clust.ext" & metric!= "REC" & model!="PanBcell" & dataset!="Abdelaal")

results.to.plot$dataset <- factor(results.to.plot$dataset,levels = c("Hao","Jerby","Yost","Zilionis","Abdelaal"))
gb = results.to.plot%>%group_by(method,metric)
global.perf <- summarise(gb,value = mean(value,na.rm =T))%>%arrange(desc(value))

ordered.methods <- (summarise(gb,value = mean(value,na.rm =T))%>%subset(metric=="MCC")%>%arrange(desc(value)))$method
#ordered.methods <- c("scGate","SingleR","Garnett","SCINA")

global.perf$method <- factor(global.perf$method, levels = ordered.methods)
global.perf$metric <- factor(global.perf$metric, levels = c("PREC","REC","MCC"))
  
colors <- c("#56B4E9","#E69F00","#56E9BA","#E06D55")
#colors <- c("#56B4E9","#E8E047","#E06D55","#E69F00")
global.plt <- ggplot() +
  geom_bar(data=global.perf, aes(x=metric, y=value, fill=method), stat="identity", position="dodge", width = 0.75, alpha=0.3) +
#  geom_jitter(data=global.perf, aes(x=metric, y=value, color = method)) + 
  scale_color_manual(values=colors) + scale_fill_manual(values=colors) +
  ylab("Global Performance")   + theme_bw() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.4, hjust=0.5)) 

global.plt
```



Summarize performance
```{r,fig.width = 8,fig.height=5}
data_summary <- function(x) {
     m <- mean(x)
     se <- function(x) sqrt(var(x)/length(x))
     ymin <- m-se(x)
     ymax <- m+se(x)
     return(c(y=m,ymin=ymin,ymax=ymax))
}

  
#res <- results.to.plot%>%subset(metric %in% c("PREC","MCC") & model!= "Plasma_cell")
res <- results.to.plot%>%subset(metric %in% c("PREC","MCC") & ! (model %in% c("Plasma_cell","PanBcell")))

  
res.prec <- res%>%subset(metric == "PREC")
res.prec$method <- factor(res.prec$method, levels = ordered.methods)
res.mcc <- res%>%subset(metric == "MCC")
res.mcc$method <- factor(res.mcc$method, levels = ordered.methods)

```


And visualize performance metrics in one plot
```{r}
res.prec <- res.prec[!is.na(res.prec$value),]

levs <- c("Bcell","MoMacDC","Myeloid","NK","Tcell","CD4T","CD8T")

res.prec$model <- factor(res.prec$model, levels=levs)   #reorder in more intuitive way

smm.prec <- aggregate(res.prec$value, list(res.prec$method, res.prec$model), FUN=mean)
colnames(smm.prec) <- c("method","model","value")

plt.prec <- ggplot() +
  geom_bar(data=smm.prec, aes(x=model, y=value, fill=method), stat="identity", position="dodge", width = 0.75, alpha=0.3) +
  geom_jitter(data=res.prec, aes(x=model, y=value, color = method, shape = dataset), 
              position=position_jitterdodge(jitter.width = 0.1), size = 1.5) +
  scale_color_manual(values=colors) + scale_fill_manual(values=colors) +
  ylab("Precision")   + theme_bw() + theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.4, hjust=0.5)) 
  

res.mcc <- res.mcc[!is.na(res.mcc$value),]
res.mcc$model <- factor(res.mcc$model, levels=levs)   #reorder in more intuitive way

smm.mcc <- aggregate(res.mcc$value, list(res.mcc$method, res.mcc$model), FUN=mean)
colnames(smm.mcc) <- c("method","model","value")


plt.mcc <- ggplot() +
  geom_bar(data=smm.mcc, aes(x=model, y=value, fill=method), stat="identity", position="dodge", width = 0.75, alpha=0.3) +
  geom_jitter(data=res.mcc, aes(x=model, y=value, color = method, shape = dataset), 
              position=position_jitterdodge(jitter.width = 0.1), size = 1.5) +
  scale_color_manual(values=colors) + scale_fill_manual(values=colors) +
  ylab("MCC")   + theme_bw() +
#  scale_shape_manual(values=1:length(unique(res.prec$dataset))) +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.4, hjust=0.5)) 

a <- plt.prec | plt.mcc
b <- plt.prec / plt.mcc

a

#remove x axis labels
plt.prec <- plt.prec + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

b <- plt.prec / plt.mcc
b

ggsave(file.path(figure.path,"benchmark_24012022_scGate_portr.pdf"), plot=b, width=6.5, height=4)
```


```{r}
saveRDS(results,file = file.path(figure.path,"results_benchmark_24012022_scGate.rds"))  
```


## Statistical comparison
```{r}
Methods <- res.mcc$method%>%unique()

mcc.mean <- list()
mcc.pvs <- list()
for(met in Methods){
    mcc.mean[[met]] <- mean(res.mcc[res.mcc$method == met,"value"],drop.na=T)
    if(met!="scGate"){
      mcc.pvs[[met]] <- wilcox.test(res.mcc[res.mcc$method == "scGate","value"],res.mcc[res.mcc$method == met,"value"],paired = T)$p.value    
    }
}

prec.mean <- list()
prec.pvs <- list()
for(met in Methods){
    prec.mean[[met]] <- mean(res.prec[res.prec$method == met,"value"],drop.na=T)
    if(met!="scGate"){
      prec.pvs[[met]] <- wilcox.test(res.prec[res.prec$method == "scGate","value"],res.prec[res.prec$method == met,"value"],paired = T)$p.value    
    }
}
```


```{r}
print("mean MCC")
mcc.mean%>%unlist()
print("mean PREC")
prec.mean%>%unlist()

print("Adjusted p-values (MCC)")
mcc.pvs%>%unlist()%>%p.adjust(.,method = "BH")

print("Adjusted p-values (PREC)")
prec.pvs%>%unlist()%>%p.adjust(.,method = "BH")

```
```{r}
sessionInfo()
```
